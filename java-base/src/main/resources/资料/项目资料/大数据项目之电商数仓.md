# 电商数仓

## 整体讲解

接下来我将为您介绍我独立负责的【电商数据仓库】项目。

​    这个项目的核心目标，是为一个电商业务构建一个**支持离线**的数据中台。它整合了来自用户前端的行为数据（比如点击、浏览）和后端业务数据库的交易数据（比如订单、支付），最终转化为可供决策的商业报表，比如用户留存分析、商品复购率等。

**整体架构与流程**

项目采用经典的分层数据仓库架构，数据流转如同一个加工流水线：

- **数据采集层**：我们使用**Flume**实时采集用户行为日志到**Kafka**；同时，使用**Maxwell**工具实时监听MySQL的**binlog**（二进制日志），将业务数据的变更也捕获到Kafka。这样就形成了一个统一的数据入口。
- **数据存储与计算层**：数据从Kafka出发，进入**HDFS**，并基于**Hive**进行离线计算。整个数仓分为五层：
  - **ODS层**存放原始数据；
  - **DIM层**是维度表，比如商品、用户信息；
  - **DWD层**是明细事实表，清晰记录每个业务事件，如下单、支付；
  - **DWS层**是对DWD层数据的轻度汇总，提前计算好常用指标；
  - **ADS层**则是直接面向业务的应用数据。
- **数据应用层**：最终，ADS层的数据通过**DataX**工具导出到MySQL，再由**Superset**进行可视化展示。

**我的核心工作与亮点**

在这个项目中，我不仅实现了端到端的搭建，更重点解决了几个关键问题：

1. **数据建模方面**：我采用了**维度建模**方法。比如，为了高效处理会变化的用户信息，我设计了**拉链表**，既节省了存储空间，又能追溯任意时间点的用户状态。
2. **数据同步方面**：我设计了**增量与全量相结合的同步策略**。对于每天变化不大的数据（如商品信息），使用DataX做每日全量同步；对于变化频繁的数据（如订单），则通过Maxwell解析binlog进行实时增量同步，并解决了**首日全量初始化**的技术难点。
3. **数据质量与性能优化**：我编写了**Flume拦截器**来解决日志时间戳的**“零点漂移”** 问题，确保数据时间准确。同时，通过配置合并小文件、选择合适的压缩格式，有效提升了HDFS的存储和计算性能。
4. **自动化调度**：我使用**DolphinScheduler**将数据同步、分层计算、报表导出等任务串联成自动化工作流，实现了每日定时调度，保证了数据产出的稳定和高效。

**项目成果与个人收获**

最终，这个数仓项目成功支撑了公司多个主题的分析需求，将数据产出时间从小时级缩短到分钟级，提升了决策效率。

对我个人而言，通过这个项目，我不仅熟练掌握了大数据生态组件（如Hadoop、Kafka、Hive）的集成与调优，更深刻理解了从**数据采集、清洗、建模到可视化**的全链路开发思想，具备了独立构建企业级数据仓库的能力。



## **具体方面**

## **1. 数据仓库概念**

- **数据仓库**：为企业决策提供数据支持，改进业务、提高效率。
- **数据来源**：业务数据（下单/支付）、用户行为数据（点击/浏览/评论）、爬虫数据。

------

## 2. 项目需求与架构设计

- **需求**：
  1. 搭建用户行为数据采集平台
  2. 搭建业务数据采集平台
  3. 支持离线分析和实时分析
- **架构设计**：
  - 技术选型（Hadoop、Kafka、Flume、Spark、Flink、Hive、HBase等）。
  - 集群分为生产集群和测试集群，按 Master/Core/Common 节点部署。
  - 服务器资源规划，合理分配不同组件。

------

## 3. 用户行为日志

  主要包括用户的各项**行为信息**以及行为所处的**环境信息**。收集这些信息的主要目的是优化产品和为各项分析统计指标提供数据支撑。收集这些信息的手段通常为**埋点**。

  本项目收集和分析的用户行为信息主要有***\*页面浏览记录、动作记录、曝光记录、启动记录和错误记录。\****

- **日志类型**：页面浏览、动作、曝光、启动、错误。

  **页面浏览记录**，记录的是**访客对页面的浏览行为**，该行为的环境信息主要有用户信息、时间信息、地理位置信息、设备信息、应用信息、渠道信息及页面信息等。

  **动作记录**，用户在页面上的操作行为。

  **曝光行为，**某个商品或广告 **在页面上被展示** 的行为，不管用户有没有点击。

  **启动记录，**记录用户 **启动应用** 的行为。

- **埋点方式**：代码埋点、可视化埋点、全埋点。

  ***\*代码埋点\****是通过调用埋点SDK函数，在需要埋点的业务逻辑功能位置调用接口，上报埋点数据。例如，我们对页面中的某个按钮埋点后，当这个按钮被点击时，可以在这个按钮对应的 OnClick 函数里面调用SDK提供的数据发送接口，来发送数据。

  ***\*可视化埋点\****只需要研发人员集成采集 SDK，不需要写埋点代码，业务人员就可以通过访问分析平台的“圈选”功能，来“圈”出需要对用户行为进行捕捉的控件，并对该事件进行命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集 SDK 按照圈选的配置自动进行用户行为数据的采集和发送。

  ***\*全埋点\****是通过在产品中嵌入SDK，前端自动采集页面上的全部用户行为事件，上报埋点数据，相当于做了一个统一的埋点。然后再通过界面配置哪些数据需要在系统里面进行分析。

  

- **日志结构**：
  
  - 页面日志（包含浏览、动作、曝光、报错、环境信息）。
  - 启动日志（启动信息 + 报错信息 + 环境信息）。
  
- **数据模拟**：提供 `jar` 包和配置文件，可生成测试日志，并支持批量脚本运行。

------

 **用户行为数据采集模块**

- **数据通道**：日志 → Flume → Kafka → HDFS/Hive …

  采集的日志存储在Hadoop2上，在该节点配置采集Flume，并对日志格式（JSON）进行校验，然后将校验通过的日志发送到Kafka。

- **环境准备**：
  
  - 脚本（xsync、xcall）用于批量分发/执行命令。
  - Hadoop、Zookeeper、Kafka、Flume 的安装与优化经验。
  
- **Flume 日志采集**：
  - 使用 `TailDirSource + KafkaChannel`，实现断点续传和高效采集。
  - 提供完整的 Flume 配置文件和启停脚本。

1. **理解日志采集的流程**（埋点 → 日志 → Flume → Kafka → HDFS/Hive）。
2. **掌握集群环境搭建与优化**（Hadoop、Kafka、Zookeeper、Flume）。
3. **熟悉日志格式与模拟数据生成**，能为后续的离线分析和实时计算打好基础。



## 电商业务简介

- **业务流程**：用户浏览 → 加入购物车 → 登录/结算 → 生成订单 → 支付 → 发货/订单跟踪 → 完成。
- **涉及环节**：商品详情、购物车、用户中心、支付、订单后台等，最终会形成大量业务数据表。
- **核心概念**：
  - **SKU**（库存量单位）：最小销售单元，如一台银色 128G 的 iPhoneX。
  - **SPU**（标准产品单元）：一类商品的集合，如“iPhoneX”。
  - 平台属性（系统维度，如颜色、尺寸）和销售属性（与销售直接相关，如价格、配置）。

------

##  业务数据介绍

- **数据库表结构**：文档列出了 **34 张核心业务表**，涵盖商品、订单、用户、营销、支付等多个维度。
- 主要包括：
  - 活动相关：活动表、活动规则表、活动商品表
  - 商品相关：SKU 表、SPU 表、属性表、分类表、品牌表
  - 用户相关：用户信息表、用户地址表、收藏表、购物车表
  - 订单相关：订单表、订单明细表、订单状态表、退单表
  - 支付相关：支付表、退款表、支付流水表
  - 营销相关：优惠券信息、优惠券使用表、营销坑位表、营销渠道表
- **数据模拟**：
  - 使用 MySQL 搭建 `gmall` 数据库。
  - 提供建表 SQL 脚本（gmall.sql），编码为 UTF-8。
  - 可用 **EZDML 工具**来建模和可视化表关系。

------

## 业务数据采集模块

![image-20250923162127556](C:\Users\55422\AppData\Roaming\Typora\typora-user-images\image-20250923162127556.png)

- **采集通道**：业务数据库 → Maxwell → Kafka → 下游存储（HDFS/Hive/ClickHouse）。

  Maxwell将业务数据采集到Kafka,实时数仓和离线数仓从Kafka消费数据分别计算。

- **采集工具**：
  
  - **Maxwell**：一款轻量级的 MySQL 数据同步工具，核心功能是实时解析 MySQL 的 binlog**（二进制日志）**  文件，将数据的增删改操作转换为结构化的 JSON 格式，并推送至 Kafka 等消息中间件，实现数据的实时流转。
  
- **流程**：
  1. 启动 ZooKeeper、Kafka、Maxwell。
  2. Kafka 创建 `topic_db`，接收数据库变更数据。
  3. 通过数据模拟脚本生成业务数据（如购物车更新、订单生成）。
  4. Maxwell 捕获 binlog → 发送 JSON 消息到 Kafka。
  5. Kafka 消费端验证是否收到消息。

## 

这一部分的重点是 **业务数据采集**：

1. 熟悉电商的核心业务流程和数据表（订单、商品、用户、营销、支付）。
2. 搭建 MySQL → Maxwell → Kafka 的实时采集通道。
3. 使用 Maxwell 把数据库的增删改转为 JSON 流式写入 Kafka，为后续数仓建模和实时计算提供基础数据。

## 数仓数据同步策略

### **实时数仓同步**

- **方式**：使用 **Flink** 从 **Kafka** 实时读取数据并计算。
- **特点**：**无需手动同步**，数据自动流入实时数仓。

------

### **离线数仓同步**

离线数仓分为两类数据同步：**用户行为数据** 和 **业务数据**。

![image-20250923162259809](C:\Users\55422\AppData\Roaming\Typora\typora-user-images\image-20250923162259809.png)

#### 用户行为数据同步

- **数据通道**：采用 Flume 作为同步工具，从 Kafka 的 topic_log 主题直接同步数据到 HDFS。因离线数仓用 Hive 分区表按天统计，HDFS 目标路径需包含日期层级，以区分不同日期的数据。

  ![image-20250923162431858](C:\Users\55422\AppData\Roaming\Typora\typora-user-images\image-20250923162431858.png)

- Flume 配置与优化
  - **核心组件选择**：Source 为 KafkaSource（读取 Kafka 数据）、Channel 为 FileChannel（存储数据，支持多硬盘路径配置以提升吞吐量）、Sink 为 HDFSSink（写入 HDFS）。
  - **关键配置**：指定 Kafka 集群地址、topic 名称，设置批处理大小（batchSize=5000）和批处理时间（batchDurationMillis=2000）；HDFS 路径按`/origin_data/gmall/log/topic_log/%Y-%m-%d`格式配置，文件前缀为 log，采用 gzip 压缩，还需优化小文件问题（设置 rollSize=128M、rollInterval=3600 秒，避免过多小文件占用 Namenode 内存、影响计算性能）。
  - **拦截器开发**：编写 TimestampInterceptor 类，解决零点漂移问题，将 header 中 timestamp 字段替换为日志生成的时间戳，确保数据时间准确性。
  
- 测试与脚本
  - **测试步骤**：依次启动 Zookeeper、Kafka、HDFS，再启动日志采集 Flume 和日志消费 Flume，生成模拟数据后，检查 HDFS 目标路径是否出现数据。
  - **启停脚本**：创建 f2.sh 脚本，实现 hadoop104 节点日志消费 Flume 的一键启动（ssh 远程执行启动命令并后台运行）和停止（通过进程查询与过滤杀死对应进程）。

#### 业务数据同步

分为**全量同步**和**增量同步**两种策略：

##### ✅ 全量同步

就是每天都将业务数据库中的全部数据同步一份到数据仓库，这是保证两侧数据同步的最简单的方式。

- **工具**：**DataX**
- **适用场景**：数据量小、变化少的表（如维度表）
- **流程**：每天从 MySQL 全量抽取数据到 HDFS
- **自动化**：通过脚本批量生成和执行 DataX 任务

DataX 是阿里巴巴开源的一款异构数据源离线同步工具，能够实现**不同数据库**（如 MySQL、Oracle）、**文件系统**（如 HDFS、本地文件）之间的数据同步。它采用 “插件化” 架构，通过统一的数据交换协议，屏蔽了不同数据源的差异，让用户无需关注具体数据源的语法细节，就能快速实现数据迁移或同步。

##### ✅ 增量同步

就是每天只将业务数据中的新增及变化数据同步到数据仓库。采用每日增量同步的表，通常需要在首日先进行一次全量同步。

- **工具**：**Maxwell**（捕获 MySQL binlog） + **Flume**（写入 HDFS）
- **适用场景**：数据量大、变化频繁的表（如订单、用户行为表）
- **流程**：
  - Maxwell 将 binlog 发送到 Kafka
  - Flume 从 Kafka 消费并写入 HDFS（按表和日期分区）
- **特点**：
  - 可捕获中间状态（如多次更新）
  - 支持首日全量同步（bootstrap 功能）

1. **全量表数据同步实操**
   - **DataX 部署与配置**：先部署 DataX 环境，再通过配置文件生成器（上传到服务器并修改 configuration.properties，指定 MySQL 和 HDFS 参数）批量生成全量表配置文件，配置文件中 HDFS 路径通过`${targetdir}`动态传入日期参数。
   - **测试与脚本**：手动创建 HDFS 目标路径后，执行 DataX 同步命令测试；编写 mysql_to_hdfs_full.sh 脚本，支持指定日期同步，自动处理目标路径（不存在则创建，存在则清空以保证任务可重复执行），还可一键同步单张表或所有 17 张全量表数据。
2. **增量表数据同步实操**
   - **数据通道与 Flume 配置**：Maxwell 将 MySQL 变更数据写入 Kafka 的 topic_db 主题，再由 Flume（配置 KafkaSource、FileChannel、HDFSSink）同步到 HDFS，Flume 需通过拦截器（TimestampAndTableNameInterceptor）提取表名和时间戳（转换为毫秒级，适配 HDFSSink 要求），HDFS 路径按`/origin_data/gmall/db/%{tableName}_inc/%Y-%m-%d`格式配置，区分不同表、不同日期的增量数据。
   - **Maxwell 配置优化**：修改 Maxwell 源码增加 mock_date 参数，在 config.properties 中配置该参数为业务日期，确保 Maxwell 输出数据的时间戳与业务日期一致；还需配置 Kafka 集群地址、MySQL 连接信息、过滤无需采集的表（如 gmall.z_log）。
   - **首日全量同步与脚本**：增量表首日需用 Maxwell 的 bootstrap 功能全量同步，编写 mysql_to_kafka_inc_init.sh 脚本，支持一键初始化单张或所有增量表数据；同步前需清理 HDFS 历史增量数据，同步后检查目标路径数据是否生成。
3. **集群启停脚本**：创建 cluster.sh 脚本，整合 Zookeeper、Hadoop、Kafka、Flume（采集、日志消费、业务消费）、Maxwell 的启停命令，实现集群一键启动和停止，停止时会循环等待 Kafka 进程全部停止后再关闭 Zookeeper。

### 数仓环境准备（Hive 安装部署）

1. **安装步骤**：将 hive-3.1.3.tar.gz 解压到指定目录并改名，修改 /etc/profile.d/my_env.sh 添加 HIVE_HOME 环境变量，解决日志 Jar 包冲突（重命名 log4j-slf4j-impl-2.17.1.jar）。
2. **元数据配置到 MySQL**：拷贝 MySQL 的 JDBC 驱动到 Hive 的 lib 目录，在 hive-site.xml 中配置 MySQL 连接 URL、驱动类名、用户名、密码，以及 Hive 元数据存储路径（/user/hive/warehouse）、HiveServer2 端口（10000）等参数。
3. **初始化与启动**：登录 MySQL 创建 metastore 元数据库，执行`schematool -initSchema -dbType mysql -verbose`初始化 Hive 元数据库，修改元数据库字符集（COLUMNS_V2 的 COMMENT 字段、TABLE_PARAMS 的 PARAM_VALUE 字段改为 utf-8，支持中文注释）；之后可通过`hive`命令启动 Hive 客户端，查看数据库验证安装结果。

------

## 学习重点

| 模块             | 技术栈                         | 关键点                           |
| :--------------- | :----------------------------- | :------------------------------- |
| 实时同步         | Flink + Kafka                  | 流式处理，无需同步               |
| 用户行为日志同步 | Flume + Kafka + HDFS           | 拦截器、小文件优化               |
| 业务数据全量同步 | DataX + MySQL + HDFS           | JSON配置、路径动态传参           |
| 业务数据增量同步 | Maxwell + Kafka + Flume + HDFS | binlog解析、首日全量、时间戳处理 |
| 环境与脚本       | Shell + Hive + MySQL           | 元数据管理、一键启停脚           |









# 一、数据仓库基础理论

## （一）核心概念

数据仓库是为数据分析设计的企业级数据管理系统，可整合多源数据、存储历史数据，支持企业决策与业务分析。

## （二）建模方法论

1. ER 模型（Bill Inmon）
   - 从全企业视角，用 “实体 - 关系” 抽象数据，遵循数据库规范化（如 1NF、2NF、3NF），减少数据冗余、保证一致性。
   - 缺点：物理表数量多、结构松散，不直接适用于分析统计。
2. 维度模型（Ralph Kimball）
   - 以数据分析为核心，用 “事实表 + 维度表” 呈现业务：
     - **事实表**：围绕**业务过程**（如下单、支付），存储可累加的度量值（如订单金额、商品数量）及维度外键。
     - **维度表**：围绕**业务环境**（如用户、商品、日期），存储描述性属性（如用户姓名、商品分类）。
   - 优点：结构清晰简洁，支持快速复杂查询，适配数仓分析场景。

# 二、维度建模核心组件

## （一）事实表

**1. 分类与特点**

| 类型           | 核心特点                                             | 适用场景                                                     |
| -------------- | ---------------------------------------------------- | ------------------------------------------------------------ |
| 事务型事实表   | 记录业务过程的原子操作（最细粒度），列少行多、增速快 | 分析业务过程细节指标（如下单次数、支付金额）                 |
| 周期快照事实表 | 按固定时间间隔（如每日）记录状态 / 存量数据          | 分析存量指标（如商品库存、账户余额）、连续状态指标（如温度） |
| 累积快照事实表 | 记录业务流程中多个关键里程碑（如下单→支付→发货）     | 分析里程碑间时间间隔（如下单到支付的平均时长）               |

**2. 设计流程（以事务型为例）**

1. 选择业务过程（如下单、加购）；

2. 声明粒度（如 “一个订单中的一个商品项”）；

3. 确认维度（如用户、商品、日期）；

4. 确认事实（如订单金额、商品数量）。

     第一步选择业务过程可以确定有哪些事务型事实表，第二步可以确定每张事务型事实表的每行数据是什么，第三步可以确定每张事务型事实表的维度外键，第四步可以确定每张事务型事实表的度量值字段。

## （二）维度表

**1. 设计步骤**

1. 确定维度表：每个与事实表相关的维度对应一张维度表，属性少的维度可 “维度退化”（直接嵌入事实表）；
2. 确定主维表与相关维表：主维表决定维度粒度（如商品维度主维表为`sku_info`），相关维表补充属性（如`spu_info`、`base_category`）；
3. 确定维度属性：优先选择丰富、明确的文字描述（避免编码），沉淀通用属性（如拼接字段）。

**2. 关键设计要点**

- **规范化与反规范化**：维度表需反规范化（采用星型模型），减少 Join 操作，提升查询性能（雪花模型需频繁关联，不适合分析）；

- 维度变化处理

  - 全量快照表：每日保存全量维度数据，简单易维护，但浪费存储；
  - 拉链表：记录维度历史状态（含`start_date`/`end_date`），高效存储历史数据，适用于用户信息等缓慢变化维度；适用与数据会发生变化，但变化频率不高的维度。
  
- **多值维度 / 属性**：多值维度（如一个订单含多个商品）可降低事实表粒度；多值属性（如商品多属性）可存储为`key:value`格式或多字段。

# 三、数据仓库分层设计与开发

数仓采用分层架构，每层职责明确，数据从下至上逐步加工，最终支持报表分析。

## （一）分层规划与核心职责

| 分层                 | 核心职责                                   | 存储格式 / 命名规范                                          | 关键操作                                                     |
| -------------------- | ------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ODS 层（原始数据层） | 存储从业务系统同步的原始数据，保留全量历史 | gzip 压缩；表名：`ods_表名_[inc/full]`（inc 增量，full 全量） | 从 Kafka/HDFS 同步数据，按天分区                             |
| DIM 层（维度层）     | 存储维度模型的维度表，支持事实表关联       | ORC 列式存储 + snappy 压缩；表名：`dim_表名_[full/zip]`（zip 拉链表） | 整合多源维度数据（如商品维度关联品类、品牌表），处理维度变化 |
| DWD 层（明细事实层） | 存储维度模型的事实表，保留业务过程明细     | ORC+snappy；表名：`dwd_数据域_表名_[inc/full]`               | 从 ODS 层提取业务过程明细（如下单、支付），关联维度外键      |
| DWS 层（汇总层）     | 基于 DWD 层聚合公共派生指标，减少重复计算  | ORC+snappy；表名：`dws_数据域_统计粒度_业务过程_周期`（1d/nd/td） | 按统计周期（1 日 / 7 日 / 历史至今）聚合指标（如最近 1 日各用户下单金额） |
| ADS 层（应用层）     | 面向业务需求，生成最终报表指标             | 文本格式；按业务主题分类（流量、用户、商品等）               | 基于 DWS 层计算业务指标（如用户留存率、商品复购率）          |

## （二）各层核心开发实操

## 数据仓库构建流程

（1）数据调研：业务调研和需求分析。

1.1业务调研：熟悉业务流程，熟悉业务数据。

1.2 需求分析：明确需求所需的业务过程及维度。

（2）明确数据域：根据业务情况进行纵向划分数据域，划分数据域的意义是***\*便于数据的管理和应用\****。

| ***\*数据域\**** | ***\*业务过程\****                                 |
| ---------------- | -------------------------------------------------- |
| ***\*交易域\**** | 加购、下单、取消订单、支付成功、退单、退款成功     |
| ***\*流量域\**** | 页面浏览、启动应用、动作、曝光、错误               |
| ***\*用户域\**** | 注册、登录                                         |
| ***\*互动域\**** | 收藏、评价                                         |
| ***\*工具域\**** | 优惠券领取、优惠券使用（下单）、优惠券使用（支付） |

### 1. ODS 层：原始数据接入

- **用户行为数据**：Flume 从 Kafka 的`topic_log`同步到 HDFS，配置 KafkaSource、FileChannel、HDFSSink，通过拦截器解决 “零点漂移”；
- 业务数据
  - 全量同步：用 DataX 从 MySQL 同步全量数据（如`activity_info`），生成 JSON 配置文件，动态传入日期参数；
  - 增量同步：用 Maxwell 监听 MySQL binlog，将变更数据写入 Kafka 的`topic_db`，再由 Flume 同步到 HDFS。

**ODS层的设计要点如下：**

（1）ODS层的表结构设计依托于从业务系统同步过来的数据结构。  

（2）ODS层要保存全部历史数据，故其压缩格式应选择压缩比较高的，此处选择gzip。

（3）ODS层表名的命名规范为：ods_表名_单分区增量全量标识（inc/full）。

**创建日志表**

数据仓库 **ODS 层（操作数据存储层）** 的增量日志表，核心用于存储业务系统产生的用户行为日志，是后续用户行为分析、路径分析、功能埋点分析的基础数据源。

**创建业务表**

#### 一、活动相关表

（核心记录活动基础信息与规则，均为全量表，数据变动低频，适合离线全量同步）

1. 活动信息表（全量表）：存储活动名称、时间、参与范围等基础信息

1. 活动规则表（全量表）：定义活动参与条件、优惠计算方式、限制规则等

#### 二、商品相关表

（覆盖商品从分类到属性的完整信息，以全量表为主，支撑商品维度分析）

1. 一级品类表（全量表）：商品一级分类（如 “电子产品”“服装”）

1. 二级品类表（全量表）：一级品类下的细分分类（如 “电子产品 - 手机”）

1. 三级品类表（全量表）：二级品类下的具体分类（如 “手机 - 智能手机”）

1. 品牌表（全量表）：商品品牌信息（如 “苹果”“华为”）

1. SPU 表（全量表）：商品聚合单元（如 “苹果 iPhone 15”，含品牌、型号等共性信息）

1. 商品表（全量表）：具体商品信息（可理解为 SKU 基础表，关联 SPU 与属性）

1. 商品平台属性表（全量表）：平台定义的商品公共属性（如 “重量”“尺寸”）

1. 商品销售属性值表（全量表）：商品销售相关的属性值（如 “颜色 - 黑色”“内存 - 256G”）

#### 三、用户相关表

（记录用户基础信息与行为，含全量与增量表，需区分静态信息与动态行为）

1. 用户表（增量表）：用户基础信息（如手机号、注册时间，新增 / 更新用户需增量同步）

1. 收藏表（增量表）：用户收藏商品的记录（实时新增 / 取消，需增量捕获）

1. 购物车表（全量表 + 增量表）：

- - 全量表：可能为每日静态快照，记录购物车整体状态

- - 增量表：实时捕获购物车新增、修改、删除操作（如添加商品、调整数量）

#### 四、订单相关表

（覆盖订单从创建到退单的全链路，均为增量表，需实时同步动态变化）

1. 订单表（增量表）：主订单信息（如订单号、用户 ID、下单时间、总金额）

1. 订单明细表（增量表）：订单中的商品明细（如订单下的具体 SKU、单价、数量）

1. 订单状态流水表（增量表）：订单状态变更记录（如 “待支付→已支付→已发货”）

1. 退单表（增量表）：订单退单信息（如退单原因、退单金额、退单状态）

1. 订单明细活动关联表（增量表）：订单明细与参与活动的关联（如某商品使用了 “满减活动”）

1. 订单明细优惠券关联表（增量表）：订单明细与使用优惠券的关联（如某商品抵扣了 “10 元优惠券”）

#### 五、支付相关表

（记录支付与退款的交易流水，均为增量表，需实时同步确保资金数据准确）

1. 支付表（增量表）：支付记录（如支付订单号、支付方式、支付金额、支付时间）

1. 退款表（增量表）：退款记录（如退款单号、关联支付单、退款金额、退款状态）

#### 六、营销相关表

（涵盖优惠券、营销渠道与坑位，含全量与增量表，支撑营销效果分析）

1. 优惠券信息表（全量表）：优惠券基础规则（如面额、有效期、使用门槛）

1. 优惠券领用表（增量表）：用户领取优惠券的记录（实时新增，需增量捕获）

1. 营销坑位表（全量表）：营销展示位置（如首页 Banner、活动专题页坑位）

1. 营销渠道表（全量表）：营销推广渠道（如 APP、小程序、第三方合作平台）

#### 七、通用基础表

（支撑全系统的通用编码定义，为全量表，数据几乎无变动）

1. 编码字典表（全量表）：系统通用编码与含义（如 “订单状态码 - 1 = 待支付”“用户类型 - 2 = 会员”）

1. 省份表（全量表）：省份基础信息（如 “北京”“上海”，含省份 ID、名称）

1. 地区表（全量表）：省份下的地区信息（如 “北京 - 朝阳区”“上海 - 浦东新区”）

### 2. DIM 层：维度表构建

**DIM层设计要点：**

（1）DIM层的设计依据是维度建模理论，该层存储维度模型的维度表。

（2）DIM层的数据存储格式为orc列式存储+snappy压缩。

（3）DIM层表名的命名规范为dim_表名_全量表或者拉链表标识（full/zip）

- **商品维度表（dim_sku_full）**：关联`ods_sku_info_full`、`ods_spu_info_full`、品类表等，整合商品属性、品类、品牌信息；
- 优惠卷维度表，活动维度表，地区维度表，销售坑位维度表、销售渠道维度表、                                                                                                                                                                                                                                                                                             
- **用户维度表（dim_user_zip）**：用拉链表存储用户历史状态，首日全量同步，每日更新新增 / 变更数据，标记`start_date`/`end_date`；
- **日期维度表（dim_date）**：手动导入一年日期数据（含周、月、季度等属性），无需每日更新。





### 3. DWD 层：事实表构建

**DWD层设计要点：**

（1）DWD层的设计依据是维度建模理论，该层存储维度模型的事实表。

（2）DWD层的数据存储格式为orc列式存储+snappy压缩。

（3）DWD层表名的命名规范为dwd_数据域_表名_单分区增量全量标识（inc/full）

- **事务事实表（如 dwd_trade_order_detail_inc）**：从 ODS 层提取下单明细，关联订单、活动、优惠券表，补充用户、省份等维度外键；

  交易域加购事务事实表 、交易域下单事务事实表 、交易域支付成功事务事实表 、

  工具域优惠券使用(支付)事务事实表 、

  互动域收藏商品事务事实表 、

  流量域页面浏览事务事实表 、

  用户域用户注册事务事实表 、 用户域用户登录事务事实表 

  

- **周期快照事实表（如 dwd_trade_cart_full）**：每日同步购物车全量数据，筛选未下单记录，统计存量；

  交易域购物车周期快照事实表 、

- **累积快照事实表（如 dwd_trade_trade_flow_acc）**：记录下单→支付→收货的里程碑时间，支持时间间隔分析。

  交易域交易流程累积快照事实表 、

### 4. DWS 层：指标汇总

**设计要点：**

（1）DWS层的设计参考指标体系。

（2）DWS层的数据存储格式为orc列式存储+snappy压缩。

（3）DWS层表名的命名规范为dws_数据域_统计粒度_业务过程_统计周期（1d/nd/td）。

注：1d表示最近1日，nd表示最近n日，td表示历史至今。

- **最近 1 日汇总（如 dws_trade_user_order_1d）**：按用户粒度聚合当日下单次数、金额；

  交易域用户商品粒度订单最近1日汇总表 

  交易域用户粒度订单最近1日汇总表

  交易域用户粒度加购最近1日汇总表

  交易域用户粒度支付最近1日汇总表

  交易域省份粒度订单最近1日汇总表

  工具域用户优惠券粒度优惠券使用(支付)最近1日汇总表

   互动域商品粒度收藏商品最近1日汇总表

  流量域会话粒度页面浏览最近1日汇总表

  流量域访客页面粒度页面浏览最近1日汇总表

- **最近 n 日汇总（如 dws_trade_user_sku_order_nd）**：基于 1 日汇总表，聚合最近 7/30 日指标；

  交易域用户商品粒度订单最近n日汇总表 

  交易域省份粒度订单最近n日汇总表 

- **历史至今汇总（如 dws_trade_user_order_td）**：每日合并前日汇总数据与当日新增数据，统计用户历史下单总次数、首次 / 末次下单日期。

  交易域用户粒度订单历史至今汇总表 

  用户域用户粒度登录历史至今汇总表        

### 5. ADS 层：报表生成

按业务主题生成指标，常见主题及核心指标如下：

| 主题       | 核心指标                                         | 示例                                 |
| ---------- | ------------------------------------------------ | ------------------------------------ |
| 流量主题   | 访客数、会话平均时长、会话平均浏览页面数、跳出率 | 各渠道最近 1/7/30 日流量统计         |
| 用户主题   | 新增 / 活跃用户数、留存率、流失 / 回流用户数     | 用户 7 日留存率、连续 3 日下单用户数 |
| 商品主题   | 品牌复购率、品类下单统计、购物车存量 Top3        | 最近 30 日各品牌复购率               |
| 交易主题   | 各省份订单数、下单到支付平均时长                 | 各省份最近 30 日订单金额             |
| 优惠券主题 | 优惠券使用次数、使用人数                         | 最近 1 日各优惠券使用统计            |





# 四、数据同步与报表导出

## （一）数据同步工具

| 工具    | 适用场景                           | 核心配置                                                     |
| ------- | ---------------------------------- | ------------------------------------------------------------ |
| Flume   | 用户行为数据、Maxwell 增量数据同步 | KafkaSource→FileChannel→HDFSSink，拦截器处理时间戳 / 表名    |
| DataX   | MySQL 全量数据同步到 HDFS/MySQL    | 配置 Reader（mysqlreader）、Writer（hdfswriter/mysqlwriter），动态传入路径参数 |
| Maxwell | MySQL 增量数据同步到 Kafka         | 监听 binlog，配置`mock_date`保证时间戳与业务日期一致         |

## （二）报表数据导出

1. **MySQL 建库建表**：创建`gmall_report`数据库，建立与 ADS 层对应的表（如`ads_user_retention`）；
2. **DataX 导出**：编写 DataX 配置文件，从 HDFS 读取 ADS 层数据，写入 MySQL，支持`replace`模式避免重复数据；
3. **脚本自动化**：编写`hdfs_to_mysql.sh`，批量导出所有 ADS 表数据，清理 HDFS 空文件避免报错。

# 五、工作流调度（DolphinScheduler）

DolphinScheduler 是一款开源的分布式工作流调度系统，核心用于解决大数据场景下 “多任务依赖、定时执行、可视化监控” 的问题，能将数据同步（如 DataX/Maxwell）、数据清洗、模型计算等任务按业务逻辑编排成工作流，实现自动化调度与运维。

## （一）部署模式

1. **集群模式**：启动所有进程，需在 Worker 节点分发 Hive、Spark、DataX 等依赖；
2. **单机模式**：启动`standalone-server`，无需分发依赖，适合资源有限的测试环境。

## （二）核心操作

1. **资源上传**：将数仓脚本（如`mysql_to_hdfs_full.sh`）上传到资源中心；

2. **环境配置**：创建`gmall`环境，配置 Hadoop、Spark、Hive 等环境变量；

3. 工作流设计

   - 节点配置：按 “ODS→DIM→DWD→DWS→ADS→MySQL 导出” 顺序创建节点，设置依赖关系；
   - 全局参数：设置`dt`（日期），定时调度时用`$[yyyy-MM-dd-1]`取前一天日期；
   
4. **上线与执行**：工作流上线后，手动执行或定时调度，监控任务运行状态。

# 六、关键工具与脚本总结

| 工具 / 脚本                                 | 功能                             | 核心命令 / 配置                           |
| ------------------------------------------- | -------------------------------- | ----------------------------------------- |
| Flume 启停脚本（f2.sh/f3.sh）               | 启动 / 停止日志 / 业务数据 Flume | `ssh hadoop104 "nohup flume-ng agent..."` |
| DataX 全量同步脚本（mysql_to_hdfs_full.sh） | 同步单张 / 所有全量表            | `mysql_to_hdfs_full.sh all 2022-06-08`    |
| 数仓分层脚本（ods_to_dim.sh/ods_to_dwd.sh） | ODS→DIM/DWD 层数据装载           | `ods_to_dim.sh all 2022-06-09`            |
| 报表导出脚本（hdfs_to_mysql.sh）            | ADS 层数据导出到 MySQL           | `hdfs_to_mysql.sh all`                    |

通过以上分层开发与调度部署，可构建一个自动化、可扩展的电商数据仓库，支持日常业务分析与决策支持。学习时建议结合实操，重点掌握维度建模逻辑、各层数据流转及调度配置。

## 可视化报表

Apache Superset是一个现代的数据探索和可视化平台。它功能强大且十分易用，可对接各种数据源，包括很多现代的大数据分析引擎，拥有丰富的图表展示形式，并且支持自定义仪表盘。
